// register file for simple dx8 style shader VM
//
// the packing/unpacking might be potentially be an issue on dx10 era/VLIW hardware or mobile (lol mobile)
// this should eventually be tested and changes made as needed (or even a simple alternative path with fixed shading)
// ....what I would give for packed math support or 8 bit arithmetic on older gpus *sobs*

#include "defines.gli"
#include "uniforms.gli"
#include "math.gli"
#include "isa.gli"

#ifndef REG_H
#define REG_H

#ifdef FRAGMENT_SHADER

// todo: this should be permutations
#ifdef REFRACTION
#define REG_COUNT    6
#define REG_FP_COUNT 4
#else
#define REG_COUNT    3
#define REG_FP_COUNT 1
#endif

// registers
uint  c_l;				//       16:16 - cluster index | lod bias (float)
uint  vdir;				//       16:16 - tangent space view dir
uint  r[REG_COUNT];     //     8:8:8:8 - temp registers (for color math)
uint  v[2];				//     8:8:8:8 - color registers
uvec2 vpos;				// 16:16:16:16 - view space position + depth

// something like GL_AMD_shader_explicit_vertex_parameter could
// allow us to prevent keeping vertex attributes around in VGPRs
// unfortunately I don't have that extension and can't test the results
vec2 tr[REG_FP_COUNT]; // 32:32 - texture coordinate registers

vec2 packTexcoordRegister(vec2 unpackedInput)
{
	return unpackedInput.xy;//packHalf2x16(unpackedInput.xy - 0.5);
}

vec2 read_texcoord_reg(uint i)
{
	return tr[i];//unpackHalf2x16(tr[i]).xy + 0.5;
}

uint pack_vertex_reg(vec4 unpackedInput)
{
	unpackedInput.rgb = fastExp2x3(-unpackedInput.rgb);
	return packUnorm4x8(unpackedInput);
}

vec4 unpack_vertex_reg(uint packedInput)
{
	vec4 v = unpackUnorm4x8(packedInput);
	v.rgb = -fastLog2x3(v.rgb);
	return v;
}

uint swizzle_comp(uint s_swizzle, uint idx)
{
	return (s_swizzle >> (idx << 1)) & 0x03;
}

vec4 swizzle_reg(vec4 reg, uint s_swizzle)
{
	if (s_swizzle == 0xe4) // no swizzle
		return reg;
	
	vec4 result;
	result.x = reg[swizzle_comp(s_swizzle, 0)];
	result.y = reg[swizzle_comp(s_swizzle, 1)];
	result.z = reg[swizzle_comp(s_swizzle, 2)];
	result.w = reg[swizzle_comp(s_swizzle, 3)];
	return result;
}

vec4 decode_reg(uint reg, uint s_fmt)
{
	switch(s_fmt)
	{
	default:
	case REG_UNORM8:	return unpackUnorm4x8(reg);
	case REG_SNORM8:	return unpackSnorm4x8(reg);
	case REG_RG16F:		return vec4( unpackHalf2x16(reg), 0, 0);
	case REG_R32F:		return vec4( uintBitsToFloat(reg) );
	}
}

uint encode_reg(vec4 reg, uint s_fmt)
{
	switch(s_fmt)
	{
	default:
	case REG_UNORM8:	return packUnorm4x8(reg);
	case REG_R32F:		return floatBitsToUint(reg.x);
	case REG_RG16F:		return packHalf2x16(reg.xy);
	case REG_SNORM8:	return packSnorm4x8(reg);
	}
}

vec4 fetch_reg(uint s_idx, uint s_type, uint s_imm, uint s_fmt)
{
	switch(s_type)
	{
	case REG_TYPE_GPR:	return decode_reg(r[s_idx], s_fmt);
	case REG_TYPE_CLR:	return unpack_vertex_reg(v[s_idx]);
	case REG_TYPE_CON:	return shaderConstants[s_idx];
	case REG_TYPE_TEX:	return vec4(tr[s_idx].xy, 0, 0);
	case REG_TYPE_IMM:	return vec4(unpackUnorm4x8(s_imm).x);
	default:			return vec4(0.0);
	}
}

vec4 apply_scale_bias(vec4 reg, uint s_scalebias)
{
	switch(s_scalebias)
	{
	case SRC_SCALE_BIAS_2X:		return reg * 2.0;
	case SRC_SCALE_BIAS_4X:		return reg * 4.0;
	case SRC_SCALE_BIAS_D2:		return reg * 0.5;
	case SRC_SCALE_BIAS_D4:		return reg * 0.25;
	case SRC_SCALE_BIAS_BIAS:	return reg + -0.5;
	case SRC_SCALE_BIAS_BX2:	return reg * 2.0 + (-1.0);
	default:					return reg;
	}
}

vec4 apply_neg_inv(vec4 reg, uint s_neg_inv)
{
	switch(s_neg_inv)
	{
	case SRC_NEG_INV_NEG:	return -reg; 
	case SRC_NEG_INV_INV:	return 1.0 - reg;
	default:				return reg;
	}
}

vec4 apply_neg_inv_abs(vec4 reg, uint s_neg_inv)
{
	switch(s_neg_inv)
	{
	case SRC_NEG_INV_NEG:	return -abs(reg); 
	case SRC_NEG_INV_INV:	return 1.0 - abs(reg);
	default:				return abs(reg);
	}
}

vec4 apply_reduction(vec4 reg, uint s_reduction)
{
	switch(s_reduction)
	{
	case SRC_RED_LUM:	return vec4( dot(reg.rgb, vec3(0.2125, 0.7154, 0.0721)) );
	case SRC_RED_SUM:	return vec4( reg.r + reg.g + reg.b );
	case SRC_RED_AVG:	return vec4( (reg.r + reg.g + reg.b) / 3.0 );
	case SRC_RED_MIN:	return vec4( min3(reg.r, reg.g, reg.b) );
	case SRC_RED_MAX:	return vec4( max3(reg.r, reg.g, reg.b) );
	case SRC_RED_MAG:	return vec4( fastLength(reg.rgb) );
	default:			return reg;
	}
}

vec4 read_src(vm_src s_operand)
{
	// fetch the relevant register or immediate value
	vec4 reg = fetch_reg(s_operand.idx_imm, s_operand.type, s_operand.idx_imm, s_operand.fmt);

	// apply swizzle
	reg = swizzle_reg(reg, s_operand.swizzle);
	
	// do any needed scaling and biasing
	reg = apply_scale_bias(reg, s_operand.scale_bias);
	
	// take absolute if necessary
	if( s_operand.absolute)
		reg = apply_neg_inv_abs(reg, s_operand.neg_inv);
	else
		reg = apply_neg_inv(reg, s_operand.neg_inv);

	return apply_reduction(reg, s_operand.reduction);
}

void write_reg(vm_dst s_dest, vec4 reg)
{
	// clear the masked bits
	r[s_dest.idx] &= ~s_dest.mask;

	// swizzle for the destination
	reg = swizzle_reg(reg, s_dest.swizzle);

	switch (s_dest.modifier)
	{
		case DST_MOD_X2:		reg = sat4(        reg * 2.0);	break;
		case DST_MOD_X4:		reg = sat4(        reg * 4.0);	break;
		case DST_MOD_X8:		reg = sat4(        reg * 8.0);	break;
		case DST_MOD_D2:		reg = sat4(        reg * 0.5);	break;
		case DST_MOD_D4:		reg = sat4(       reg * 0.25);	break;
		case DST_MOD_D8:		reg = sat4(      reg * 0.125);	break;
		case DST_MOD_POW2:		reg = sat4(        pow2(reg));	break;
		case DST_MOD_POW4:		reg = sat4(        pow4(reg));	break;
		case DST_MOD_SQRT:		reg = sat4(        sqrt(reg));	break;
		case DST_MOD_PACK:		reg = sat4(  reg * 0.5 + 0.5);	break;
		case DST_MOD_ABS:		reg = sat4(         abs(reg));	break;
		case DST_MOD_NEG:		reg = sat4(             -reg);	break;
		case DST_MOD_ROLLOFF:   reg = sat4(reg / (1.0 + reg));	break;
		default:				reg = sat4(              reg);	break;
	}

	// write it
	uint p = encode_reg(reg, s_dest.fmt);
	r[s_dest.idx] |= (p & s_dest.mask);
}

vec4 read_dest(vm_dst s_dst)
{
	return decode_reg(r[s_dst.idx], s_dst.fmt);
}

#endif

#endif
